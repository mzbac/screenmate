import Cocoa
import SwiftUI
import MLX
import MLXLLM
import MLXLMCommon 
import MLXVLM
import Tokenizers
import Hub 
import Logging 

enum ScreenMateEngineError: Error, LocalizedError {
    case modelNotLoaded
    case modelLoadFailed(String, Error? = nil)
    case tokenizerNotAvailable
    case eosTokenMissing
    case imageProcessingFailed(String, Error? = nil)
    case userInputPreparationFailed(String, Error? = nil)
    case processorError(String)
    case inferenceFailed(String, Error? = nil)
    case noTextGenerated
    case decodingGeneratedTextFailed

    var errorDescription: String? {
        switch self {
        case .modelNotLoaded: return "VLM model is not loaded. Please load a model first."
        case .modelLoadFailed(let msg, let err): return "Failed to load VLM model: \(msg)\(err != nil ? " (\(err!.localizedDescription))" : "")"
        case .tokenizerNotAvailable: return "Tokenizer is not available for the loaded model."
        case .eosTokenMissing: return "EOS token ID is missing in the tokenizer."
        case .imageProcessingFailed(let msg, let err): return "Failed to process image: \(msg)\(err != nil ? " (\(err!.localizedDescription))" : "")"
        case .userInputPreparationFailed(let msg, let err): return "Failed to prepare input for VLM: \(msg)\(err != nil ? " (\(err!.localizedDescription))" : "")"
        case .processorError(let msg): return "Model processor error: \(msg)"
        case .inferenceFailed(let msg, let err): return "VLM inference failed: \(msg)\(err != nil ? " (\(err!.localizedDescription))" : "")"
        case .noTextGenerated: return "VLM did not generate any text output."
        case .decodingGeneratedTextFailed: return "Failed to decode the text generated by VLM."
        }
    }
}

actor ScreenMateEngineModelProvider {
    private var currentModelContainer: ModelContainer?
    private var currentModelIdString: String? 
    private var currentLoadedModelName: String?
    private let logger: Logger
    private let fileManager = FileManager.default
    private let modelFactory: ModelFactory = VLMModelFactory.shared

    init(logger: Logger) {
        self.logger = logger
        self.logger.info("ScreenMateEngineModelProvider initialized.")
    }

    func getModel(requestedModelId: String) async throws -> (container: ModelContainer, tokenizer: any Tokenizer, loadedName: String) {
        let targetModelIdString = requestedModelId 

        if let currentContainer = currentModelContainer,
           targetModelIdString == self.currentModelIdString,
           let loadedName = self.currentLoadedModelName {
            logger.info("Returning cached VLM model: \(loadedName) (for request: \(targetModelIdString))")
            let tokenizer = await currentContainer.perform { $0.tokenizer }
            return (currentContainer, tokenizer, loadedName)
        }

        logger.info("Request for VLM model '\(targetModelIdString)', current is '\(self.currentModelIdString ?? "none")'. Unloading/Loading...")
        await unloadCurrentModel() 

        let modelConfiguration: ModelConfiguration
        let expandedPath = NSString(string: targetModelIdString).expandingTildeInPath
        var isDirectory: ObjCBool = false

        if fileManager.fileExists(atPath: expandedPath, isDirectory: &isDirectory), isDirectory.boolValue {
            logger.info("Loading VLM model from local path: \(expandedPath)")
            modelConfiguration = ModelConfiguration(directory: URL(fileURLWithPath: expandedPath))
        } else {
            logger.info("Loading VLM model from Hugging Face Hub: \(targetModelIdString)")
            modelConfiguration = modelFactory.configuration(id: targetModelIdString)
        }

        do {
            let configDisplayId = modelConfiguration.name
            logger.info("Attempting to load VLM model container for: \(configDisplayId)")
            
            let newContainer = try await modelFactory.loadContainer(configuration: modelConfiguration)
            let newTokenizer = await newContainer.perform { $0.tokenizer }
            let finalLoadedName = modelConfiguration.name

            logger.info("Model '\(finalLoadedName)' loaded successfully with processor.")

            self.currentModelContainer = newContainer
            self.currentModelIdString = targetModelIdString 
            self.currentLoadedModelName = finalLoadedName
            logger.info("Successfully loaded VLM model: \(finalLoadedName) (from target: \(targetModelIdString))")
            return (newContainer, newTokenizer, finalLoadedName)
        } catch {
            logger.error("Failed to load VLM model '\(targetModelIdString)': \(error.localizedDescription)")
            await unloadCurrentModel()
            throw ScreenMateEngineError.modelLoadFailed("Loading VLM model '\(targetModelIdString)' failed.", error)
        }
    }

    func getCurrentLoadedModelName() async -> String? {
        return currentLoadedModelName
    }
    
    func hasLoadedModel() async -> Bool {
        return currentModelContainer != nil
    }

    func unloadCurrentModel() async {
        if currentModelContainer != nil {
            logger.info("Unloading current VLM model: \(currentLoadedModelName ?? "unknown")")
            currentModelContainer = nil
            currentModelIdString = nil
            currentLoadedModelName = nil
            MLX.GPU.clearCache() 
            logger.info("Cleared MLX GPU cache after unloading VLM model.")
        }
    }
}

public class ScreenMateEngine: ObservableObject {
    @Published var isLoadingModel: Bool = false
    @Published var imageProcessingInProgress: Bool = false
    @Published var currentStatusMessage: String = "ScreenMate Engine Idle."
    @Published var loadedModelNameDisplay: String = "No VLM Model Loaded"
    @Published var isModelReady: Bool = false

    private var modelProvider: ScreenMateEngineModelProvider
    private let logger: Logger

    private let defaultVLMModelIdentifier = "mlx-community/Qwen2-VL-2B-Instruct-8bit"
    
    private let ocrSystemPrompt = "You are an expert OCR assistant. Analyze the provided image and extract all visible text content accurately."
    private let ocrUserPrompt = "Extract all text from this image. Provide only the recognized text, exactly as it appears, maintaining formatting if possible. Do not add any extra commentary or conversational phrases."

    private let analysisSystemPrompt = "You are an expert image analysis assistant. Analyze the provided image and respond to the user's request accurately."
    private let analysisUserPrompt = "Extract all text from this image. Provide only the recognized text, exactly as it appears, maintaining formatting if possible. Do not add any extra commentary or conversational phrases."
    
    private let systemPrompt = "You are an expert OCR assistant. Analyze the provided image and extract all visible text content accurately."
    private let ocrUserPromptLegacy = "Extract all text from this image. Provide only the recognized text, exactly as it appears, maintaining formatting if possible. Do not add any extra commentary or conversational phrases."
    
    static let supportedVLMModels: [String: String] = [
        "Qwen2-VL 2B Instruct (8-bit) 2.35GB": "mlx-community/Qwen2-VL-2B-Instruct-8bit",
        "Qwen2-VL 2B Instruct (4-bit) 1.25GB": "mlx-community/Qwen2-VL-2B-Instruct-4bit",
        "Qwen2-VL 7B Instruct (8-bit) 8.81GB": "mlx-community/Qwen2-VL-7B-Instruct-8bit",
        "Qwen2-VL 7B Instruct (4-bit) 4.67GB": "mlx-community/Qwen2-VL-7B-Instruct-4bit",
        "SmolVLM2 500M 1.02GB": "mlx-community/SmolVLM2-500M-Video-Instruct-mlx"
    ]

    init() {
        self.logger = Logger(label: "com.github.mzbac.screenmate.ScreenMateEngine")
        self.modelProvider = ScreenMateEngineModelProvider(logger: logger)
        
        DispatchQueue.main.async {
            self.currentStatusMessage = "ScreenMate Engine ready. Load a VLM model."
            self.logger.info("ScreenMateEngine (Direct VLM): Initialized.")
        }
        Task {
            await loadModel(modelIdentifier: defaultVLMModelIdentifier)
        }
    }
    
    func getDefaultOCRPrompt() -> String {
        return ocrUserPromptLegacy
    }
    
    func getPredefinedSystemPrompt(for promptSet: AppSettings.PromptSet) -> String {
        switch promptSet {
        case .ocr:
            return ocrSystemPrompt
        case .analysis:
            return analysisSystemPrompt
        }
    }
    
    func getPredefinedUserPrompt(for promptSet: AppSettings.PromptSet) -> String {
        switch promptSet {
        case .ocr:
            return ocrUserPrompt
        case .analysis:
            return analysisUserPrompt
        }
    }
    
    func getEffectivePrompt(from appSettings: AppSettings) -> String {
        let customPrompt = appSettings.lastCustomPrompt.trimmingCharacters(in: .whitespacesAndNewlines)
        let effectivePrompt = customPrompt.isEmpty ? getPredefinedUserPrompt(for: appSettings.selectedPromptSet) : customPrompt
        return effectivePrompt
    }
    
    func getEffectiveSystemPrompt(from appSettings: AppSettings) -> String {
        let customSystemPrompt = appSettings.lastCustomSystemPrompt.trimmingCharacters(in: .whitespacesAndNewlines)
        let effectiveSystemPrompt = customSystemPrompt.isEmpty ? getPredefinedSystemPrompt(for: appSettings.selectedPromptSet) : customSystemPrompt
        return effectiveSystemPrompt
    }
    
    func hasActiveCustomPrompt(from appSettings: AppSettings) -> Bool {
        let hasActiveUserPrompt = !appSettings.lastCustomPrompt.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty
        let hasActiveSystemPrompt = !appSettings.lastCustomSystemPrompt.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty
        return hasActiveUserPrompt || hasActiveSystemPrompt
    }

    @MainActor
    func loadModel(modelIdentifier: String) async {
        guard !isLoadingModel else {
            currentStatusMessage = "Model loading already in progress."
            logger.info("\(currentStatusMessage)")
            return
        }
        
        let currentLoadedName = await modelProvider.getCurrentLoadedModelName()
        if currentLoadedName == modelIdentifier || 
           (currentLoadedName != nil && modelIdentifier == defaultVLMModelIdentifier && currentLoadedName == defaultVLMModelIdentifier) {
             if await modelProvider.hasLoadedModel() {
                isModelReady = true
                loadedModelNameDisplay = currentLoadedName! 
                currentStatusMessage = "Model '\(currentLoadedName!)' is already loaded."
                logger.info("\(currentStatusMessage)")
                return
             }
        }

        isLoadingModel = true
        isModelReady = false
        currentStatusMessage = "Loading VLM model: \(modelIdentifier)..."
        loadedModelNameDisplay = "Loading..."
        logger.info("Attempting to load VLM model: \(modelIdentifier)")

        do {
            let (_, _, loadedName) = try await modelProvider.getModel(requestedModelId: modelIdentifier)
            
            isModelReady = true
            loadedModelNameDisplay = loadedName
            currentStatusMessage = "VLM model '\(loadedName)' loaded successfully."
            logger.info("\(currentStatusMessage)")
        } catch let error as ScreenMateEngineError {
            currentStatusMessage = "Model load failed: \(error.localizedDescription)"
            logger.error("\(currentStatusMessage)")
            loadedModelNameDisplay = "Model Load Failed"
            isModelReady = false
        } catch {
            currentStatusMessage = "Unexpected model load error: \(error.localizedDescription)"
            logger.error("\(currentStatusMessage)")
            loadedModelNameDisplay = "Model Load Failed"
            isModelReady = false
        }
        isLoadingModel = false
    }
    
    @MainActor
    func unloadCurrentModel() async {
        guard !isLoadingModel else {
            currentStatusMessage = "Cannot unload while a model is loading."
            logger.warning("\(currentStatusMessage)")
            return
        }
        await modelProvider.unloadCurrentModel()
        isModelReady = false
        loadedModelNameDisplay = "No VLM Model Loaded"
        currentStatusMessage = "Model unloaded. Ready to load a new model."
        logger.info("\(currentStatusMessage)")
    }

    @MainActor
    func processImage(onNSImage nsImage: NSImage, prompt: String, completion: @escaping (Result<String, ScreenMateEngineError>) -> Void) {
        processImage(onNSImage: nsImage, prompt: prompt, customSystemPrompt: nil, completion: completion)
    }
    
    @MainActor
    func processImage(onNSImage nsImage: NSImage, prompt: String, customSystemPrompt: String?, completion: @escaping (Result<String, ScreenMateEngineError>) -> Void) {
        logger.info("Process image requested with NSImage and custom prompts.")
        logger.info("Using user prompt: '\(prompt)'")
        logger.info("Using system prompt: '\(customSystemPrompt ?? systemPrompt)'")
        
        guard isModelReady else {
            currentStatusMessage = "Error: VLM Model not loaded or not ready."
            logger.error("\(currentStatusMessage)")
            completion(.failure(.modelNotLoaded))
            return
        }
        
        guard !imageProcessingInProgress else {
            currentStatusMessage = "Image processing already in progress."
            logger.warning("\(currentStatusMessage)")
            completion(.failure(.inferenceFailed("Another image processing task is already in progress.")))
            return
        }

        imageProcessingInProgress = true
        currentStatusMessage = "Processing image with VLM..."

        Task.detached(priority: .userInitiated) {
            do {
                let modelIdToUse = await self.modelProvider.getCurrentLoadedModelName() ?? self.defaultVLMModelIdentifier
                let (modelContainer, tokenizer, _) = try await self.modelProvider.getModel(requestedModelId: modelIdToUse)

                guard let eosTokenId = tokenizer.eosTokenId else {
                    throw ScreenMateEngineError.eosTokenMissing
                }

                guard let ciImage = self.convertNSImageToCIImage(nsImage) else {
                    throw ScreenMateEngineError.imageProcessingFailed("Failed to convert NSImage to CIImage.")
                }
                self.logger.info("NSImage successfully converted to CIImage for VLM input.")

                let effectiveSystemPrompt = customSystemPrompt ?? self.systemPrompt
                let messagesForVLM: [[String: Any]] = [
                    ["role": "system", "content": effectiveSystemPrompt],
                    ["role": "user", "content": [
                        [ "type": "text", "content": prompt ],
                        [ "type": "image"]
                    ]]
                ]
                
                self.logger.info("Constructed VLM messages with system prompt: '\(effectiveSystemPrompt)' and user prompt: '\(prompt)'")
                
                let userInput = UserInput(
                    messages: messagesForVLM,
                    images: [UserInput.Image.ciImage(ciImage)]
                )
                self.logger.info("UserInput constructed with CIImage and prompts.")

                let appSettings = AppSettings.shared
                let generateParameters = GenerateParameters(
                    temperature: appSettings.temperature,
                    topP: appSettings.topP,
                    repetitionPenalty: appSettings.repetitionPenalty,
                    repetitionContextSize: appSettings.repetitionContextSize
                )

                actor TokenCollector {
                    private var tokens: [Int] = []
                    
                    func addToken(_ token: Int) {
                        tokens.append(token)
                    }
                    
                    func getCount() -> Int {
                        return tokens.count
                    }
                    
                    func getAllTokens() -> [Int] {
                        return tokens
                    }
                }
                
                let tokenCollector = TokenCollector()

                self.logger.info("Starting VLM inference via modelContainer.perform...")
                
                try await modelContainer.perform { context in
                    let processor = context.processor
                    self.logger.info("UserInputProcessor obtained. Preparing LMInput...")
                    let lmInput: LMInput = try await processor.prepare(input: userInput)
                    self.logger.info("LMInput prepared. Starting token generation...")
                    
                    let _ = try MLXLMCommon.generate(input: lmInput, parameters: generateParameters, context: context) { outputTokens in
                        guard let lastToken = outputTokens.last else { return .more }
                        if lastToken == eosTokenId { return .stop }
                        
                        Task {
                            let currentCount = await tokenCollector.getCount()
                            guard currentCount < appSettings.maxTokens else { return }
                            
                            if lastToken == tokenizer.unknownTokenId {
                                self.logger.warning("Generated unknown token ID (\(lastToken)). Skipping.")
                            } else {
                                await tokenCollector.addToken(lastToken)
                            }
                        }
                        return .more
                    }
                }
                
                let finalTokens = await tokenCollector.getAllTokens()
                self.logger.info("VLM inference completed. Generated \(finalTokens.count) tokens.")

                if finalTokens.isEmpty {
                    throw ScreenMateEngineError.noTextGenerated
                }
                
                let recognizedText = tokenizer.decode(tokens: finalTokens)
                let cleanedText = recognizedText.trimmingCharacters(in: .whitespacesAndNewlines)

                if cleanedText.isEmpty && !finalTokens.isEmpty {
                     self.logger.warning("Decoding generated tokens resulted in an empty string, though tokens were generated.")
                }
                self.logger.info("Decoded processed text (length: \(cleanedText.count)).")
                
                await MainActor.run {
                    self.imageProcessingInProgress = false
                    self.currentStatusMessage = "Image processing complete. \(finalTokens.count) tokens."
                    completion(.success(cleanedText))
                }

            } catch let screenMateError as ScreenMateEngineError {
                let errorMsg = "ScreenMate Error: \(screenMateError.localizedDescription)"
                self.logger.error("\(errorMsg)")
                await MainActor.run {
                    self.imageProcessingInProgress = false
                    self.currentStatusMessage = errorMsg
                    completion(.failure(screenMateError))
                }
            } catch {
                let errorMsg = "Unexpected image processing error: \(error.localizedDescription)"
                self.logger.error("\(errorMsg)")
                await MainActor.run {
                    self.imageProcessingInProgress = false
                    self.currentStatusMessage = errorMsg
                    completion(.failure(.inferenceFailed(errorMsg, error)))
                }
            }
        }
    }
    
    @MainActor
    func performOCR(onNSImage nsImage: NSImage, customPrompt: String, completion: @escaping (Result<String, ScreenMateEngineError>) -> Void) {
        processImage(onNSImage: nsImage, prompt: customPrompt, completion: completion)
    }
    
    private func convertNSImageToCIImage(_ nsImage: NSImage) -> CIImage? {
        if let pngData = nsImage.pngData() {
            return CIImage(data: pngData)
        }
        logger.warning("Could not get PNG data from NSImage, falling back to CGImage conversion for CIImage.")
        guard let cgImage = nsImage.cgImage(forProposedRect: nil, context: nil, hints: nil) else {
            logger.error("Failed to convert NSImage to CGImage.")
            return nil
        }
        return CIImage(cgImage: cgImage)
    }
}

extension NSImage {
    func pngData() -> Data? {
        guard let tiffRepresentation = self.tiffRepresentation,
              let bitmapImageRep = NSBitmapImageRep(data: tiffRepresentation) else {
            return nil
        }
        return bitmapImageRep.representation(using: .png, properties: [:])
    }
}
